"""
æ¨ç†é¢æ¿UI - æ”¯æŒYOLOå’ŒMediaPipeåŒä¸“æ 
æä¾›ç›®æ ‡æ£€æµ‹å’Œå…³é”®ç‚¹æ£€æµ‹çš„å¯è§†åŒ–ç•Œé¢
"""

import os
import cv2
import numpy as np
import base64
import json
import csv
from pathlib import Path
from typing import Optional, List, Dict, Any
from datetime import datetime
from dataclasses import dataclass, field
from PIL import Image, ImageDraw, ImageFont

from PyQt6.QtWidgets import (
    QWidget, QVBoxLayout, QHBoxLayout, QLabel, QPushButton,
    QComboBox, QLineEdit, QFileDialog, QGroupBox, QSpinBox,
    QDoubleSpinBox, QCheckBox, QTableWidget, QTableWidgetItem,
    QTextEdit, QSplitter, QMessageBox, QProgressBar, QTabWidget,
    QListWidget, QListWidgetItem, QFormLayout, QStackedWidget,
    QFrame, QScrollArea
)
from PyQt6.QtCore import Qt, QTimer, pyqtSignal, QThread
from PyQt6.QtGui import QImage, QPixmap, QPainter, QPen, QColor
from loguru import logger

from ..core.config_manager import ConfigManager
from ..core.yolo_inference import YOLOInference, InferenceResult


# ============== MediaPipe æ•°æ®ç±» (å…¼å®¹Python 3.11) ==============
from dataclasses import dataclass, field
from typing import List, Optional

@dataclass
class Keypoint:
    """å…³é”®ç‚¹æ•°æ®"""
    x: float  # å½’ä¸€åŒ–åæ ‡ 0-1
    y: float
    z: float = 0.0
    visibility: float = 1.0  # å¯è§åº¦ 0-1

@dataclass
class PoseResult:
    """å§¿æ€æ£€æµ‹ç»“æœ"""
    keypoints: List[Keypoint] = field(default_factory=list)
    bbox: Optional[List[float]] = None  # [x1, y1, x2, y2]
    confidence: float = 0.0

@dataclass
class HandResult:
    """æ‰‹éƒ¨æ£€æµ‹ç»“æœ"""
    keypoints: List[Keypoint] = field(default_factory=list)
    gesture: str = ""  # æ‰‹åŠ¿ç±»åˆ«
    gesture_score: float = 0.0  # æ‰‹åŠ¿ç½®ä¿¡åº¦

@dataclass
class MediaPipeResult:
    """MediaPipeæ¨ç†ç»“æœ"""
    poses: List[PoseResult] = field(default_factory=list)
    hands: List[HandResult] = field(default_factory=list)  # æ”¹ä¸ºHandResultåŒ…å«æ‰‹åŠ¿ä¿¡æ¯
    faces: List[List[Keypoint]] = field(default_factory=list)
    fps: float = 0.0
    inference_time: float = 0.0


# ============== MediaPipe å…³é”®ç‚¹æ£€æµ‹çº¿ç¨‹ (æ–°ç‰ˆtasks API) ==============
class MediaPipeThread(QThread):
    """MediaPipeå…³é”®ç‚¹æ£€æµ‹çº¿ç¨‹ - ä½¿ç”¨mediapipe.tasks API (v0.10.x)"""

    frame_ready = pyqtSignal(np.ndarray, MediaPipeResult)
    fps_updated = pyqtSignal(float)
    error_occurred = pyqtSignal(str)

    # æ¨¡å‹ä¸‹è½½URL
    MODEL_URLS = {
        'pose_landmarker_lite.task': 'https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_lite/float16/1/pose_landmarker_lite.task',
        'pose_landmarker_full.task': 'https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_full/float16/1/pose_landmarker_full.task',
        'pose_landmarker_heavy.task': 'https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task',
        'hand_landmarker.task': 'https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task',
        'face_landmarker.task': 'https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task',
        'gesture_recognizer.task': 'https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task'
    }

    # æ¨¡å‹å¤æ‚åº¦æ˜ å°„: 0=è½»é‡, 1=æ ‡å‡†, 2=é‡å‹
    POSE_MODEL_MAP = {
        0: 'pose_landmarker_lite.task',
        1: 'pose_landmarker_full.task',
        2: 'pose_landmarker_heavy.task'
    }

    def __init__(self, source_type: str, source_config: dict,
                 enable_pose: bool = True, enable_hands: bool = False,
                 enable_face: bool = False, model_complexity: int = 1,
                 enable_gesture: bool = False):
        super().__init__()
        self.source_type = source_type
        self.source_config = source_config
        self.enable_pose = enable_pose
        self.enable_hands = enable_hands
        self.enable_face = enable_face
        self.enable_gesture = enable_gesture  # å¯ç”¨æ‰‹åŠ¿è¯†åˆ«
        self.model_complexity = model_complexity  # 0=è½»é‡, 1=æ ‡å‡†, 2=é‡å‹
        self.running = False
        self.cap = None

        # MediaPipeä»»åŠ¡å¯¹è±¡
        self.pose_landmarker = None
        self.hand_landmarker = None
        self.face_landmarker = None
        self.gesture_recognizer = None  # æ‰‹åŠ¿è¯†åˆ«å™¨

        # æ¨¡å‹ç›®å½•ï¼ˆåŸå§‹ä½ç½®ï¼Œå¯èƒ½åŒ…å«ä¸­æ–‡ï¼‰
        self.models_dir = Path(__file__).parent.parent.parent / "models" / "mediapipe"
        self.models_dir.mkdir(parents=True, exist_ok=True)

        # MediaPipeå…¼å®¹ç›®å½•ï¼ˆçº¯è‹±æ–‡è·¯å¾„ï¼Œé¿å…C++åº•å±‚æ— æ³•å¤„ç†ä¸­æ–‡è·¯å¾„çš„é—®é¢˜ï¼‰
        self.compat_models_dir = Path.home() / ".mediapipe_models"
        self.compat_models_dir.mkdir(parents=True, exist_ok=True)

    def _get_model_path(self, model_name: str) -> str:
        """è·å–æ¨¡å‹è·¯å¾„ï¼Œç¡®ä¿æ˜¯çº¯è‹±æ–‡è·¯å¾„ä¾›MediaPipeä½¿ç”¨"""
        # æºæ–‡ä»¶è·¯å¾„ï¼ˆå¯èƒ½åŒ…å«ä¸­æ–‡ï¼‰
        source_path = self.models_dir / model_name

        # å¦‚æœæºæ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦ä¸‹è½½
        if not source_path.exists():
            self._download_model(model_name)

        # ç›®æ ‡è·¯å¾„ï¼ˆçº¯è‹±æ–‡è·¯å¾„ï¼‰
        target_path = self.compat_models_dir / model_name

        # å¦‚æœç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨æˆ–æºæ–‡ä»¶æ›´æ–°ï¼Œåˆ™å¤åˆ¶
        if not target_path.exists() or (
            source_path.exists() and
            source_path.stat().st_mtime > target_path.stat().st_mtime
        ):
            import shutil
            shutil.copy2(source_path, target_path)
            logger.info(f"[MediaPipeThread] æ¨¡å‹å·²å¤åˆ¶åˆ°å…¼å®¹è·¯å¾„: {target_path}")

        return str(target_path)

    def _download_model(self, model_name: str):
        """ä¸‹è½½æ¨¡å‹æ–‡ä»¶åˆ°åŸå§‹ç›®å½•"""
        model_path = self.models_dir / model_name

        url = self.MODEL_URLS.get(model_name)
        if not url:
            raise Exception(f"æœªçŸ¥çš„æ¨¡å‹: {model_name}")

        logger.info(f"[MediaPipeThread] æ­£åœ¨ä¸‹è½½æ¨¡å‹: {model_name}")

        try:
            import urllib.request
            import ssl

            # åˆ›å»ºSSLä¸Šä¸‹æ–‡
            ssl_context = ssl.create_default_context()
            ssl_context.check_hostname = False
            ssl_context.verify_mode = ssl.CERT_NONE

            # ä¸‹è½½æ–‡ä»¶
            opener = urllib.request.build_opener(
                urllib.request.HTTPSHandler(context=ssl_context)
            )
            urllib.request.install_opener(opener)

            urllib.request.urlretrieve(url, model_path)
            logger.info(f"[MediaPipeThread] æ¨¡å‹ä¸‹è½½å®Œæˆ: {model_path}")
        except Exception as e:
            logger.error(f"[MediaPipeThread] æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
            raise

    def _create_pose_landmarker(self):
        """åˆ›å»ºå§¿æ€æ£€æµ‹å™¨"""
        try:
            import mediapipe as mp
            from mediapipe.tasks.python import vision

            # æ ¹æ®æ¨¡å‹å¤æ‚åº¦é€‰æ‹©å¯¹åº”çš„æ¨¡å‹æ–‡ä»¶
            model_name = self.POSE_MODEL_MAP.get(self.model_complexity, 'pose_landmarker_full.task')
            complexity_names = {0: 'è½»é‡', 1: 'æ ‡å‡†', 2: 'é‡å‹'}
            complexity_name = complexity_names.get(self.model_complexity, 'æ ‡å‡†')
            logger.info(f"[MediaPipeThread] ä½¿ç”¨å§¿æ€æ£€æµ‹æ¨¡å‹: {complexity_name} ({model_name})")

            model_path = self._get_model_path(model_name)

            base_options = mp.tasks.BaseOptions(model_asset_path=model_path)
            options = vision.PoseLandmarkerOptions(
                base_options=base_options,
                running_mode=vision.RunningMode.VIDEO,
                num_poses=1,
                min_pose_detection_confidence=0.5,
                min_pose_presence_confidence=0.5,
                min_tracking_confidence=0.5
            )
            self.pose_landmarker = vision.PoseLandmarker.create_from_options(options)
            logger.info("[MediaPipeThread] å§¿æ€æ£€æµ‹å™¨åˆ›å»ºæˆåŠŸ")
            return True
        except Exception as e:
            logger.warning(f"[MediaPipeThread] å§¿æ€æ£€æµ‹å™¨åˆ›å»ºå¤±è´¥: {e}")
            return False

    def _create_hand_landmarker(self):
        """åˆ›å»ºæ‰‹éƒ¨æ£€æµ‹å™¨"""
        try:
            import mediapipe as mp
            from mediapipe.tasks.python import vision

            model_path = self._get_model_path('hand_landmarker.task')

            base_options = mp.tasks.BaseOptions(model_asset_path=model_path)
            options = vision.HandLandmarkerOptions(
                base_options=base_options,
                running_mode=vision.RunningMode.VIDEO,
                num_hands=2,
                min_hand_detection_confidence=0.5,
                min_hand_presence_confidence=0.5,
                min_tracking_confidence=0.5
            )
            self.hand_landmarker = vision.HandLandmarker.create_from_options(options)
            logger.info("[MediaPipeThread] æ‰‹éƒ¨æ£€æµ‹å™¨åˆ›å»ºæˆåŠŸ")
            return True
        except Exception as e:
            logger.warning(f"[MediaPipeThread] æ‰‹éƒ¨æ£€æµ‹å™¨åˆ›å»ºå¤±è´¥: {e}")
            return False

    def _create_gesture_recognizer(self):
        """åˆ›å»ºæ‰‹åŠ¿è¯†åˆ«å™¨"""
        try:
            import mediapipe as mp
            from mediapipe.tasks.python import vision

            model_path = self._get_model_path('gesture_recognizer.task')

            base_options = mp.tasks.BaseOptions(model_asset_path=model_path)
            options = vision.GestureRecognizerOptions(
                base_options=base_options,
                running_mode=vision.RunningMode.VIDEO,
                num_hands=2,
                min_hand_detection_confidence=0.5,
                min_hand_presence_confidence=0.5,
                min_tracking_confidence=0.5
            )
            self.gesture_recognizer = vision.GestureRecognizer.create_from_options(options)
            logger.info("[MediaPipeThread] æ‰‹åŠ¿è¯†åˆ«å™¨åˆ›å»ºæˆåŠŸ")
            return True
        except Exception as e:
            logger.warning(f"[MediaPipeThread] æ‰‹åŠ¿è¯†åˆ«å™¨åˆ›å»ºå¤±è´¥: {e}")
            return False

    def _create_face_landmarker(self):
        """åˆ›å»ºé¢éƒ¨æ£€æµ‹å™¨"""
        try:
            import mediapipe as mp
            from mediapipe.tasks.python import vision

            model_path = self._get_model_path('face_landmarker.task')

            base_options = mp.tasks.BaseOptions(model_asset_path=model_path)
            options = vision.FaceLandmarkerOptions(
                base_options=base_options,
                running_mode=vision.RunningMode.VIDEO,
                num_faces=1,
                min_face_detection_confidence=0.5,
                min_face_presence_confidence=0.5,
                min_tracking_confidence=0.5
            )
            self.face_landmarker = vision.FaceLandmarker.create_from_options(options)
            logger.info("[MediaPipeThread] é¢éƒ¨æ£€æµ‹å™¨åˆ›å»ºæˆåŠŸ")
            return True
        except Exception as e:
            logger.warning(f"[MediaPipeThread] é¢éƒ¨æ£€æµ‹å™¨åˆ›å»ºå¤±è´¥: {e}")
            return False

    def run(self):
        """è¿è¡Œæ¨ç†"""
        self.running = True

        try:
            # åˆå§‹åŒ–MediaPipeä»»åŠ¡
            if self.enable_pose:
                self._create_pose_landmarker()

            if self.enable_hands:
                self._create_hand_landmarker()

            if self.enable_gesture:
                self._create_gesture_recognizer()

            if self.enable_face:
                self._create_face_landmarker()

            # æ£€æŸ¥æ˜¯å¦è‡³å°‘æœ‰ä¸€ä¸ªæ£€æµ‹å™¨æˆåŠŸåˆ›å»º
            if not any([self.pose_landmarker, self.hand_landmarker, self.face_landmarker, self.gesture_recognizer]):
                raise Exception("æ²¡æœ‰å¯ç”¨çš„MediaPipeæ£€æµ‹å™¨ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å’Œç½‘ç»œè¿æ¥")

            # æ ¹æ®æºç±»å‹è¿è¡Œæ¨ç†
            if self.source_type == "camera":
                self._run_camera_inference()
            elif self.source_type == "file":
                self._run_file_inference()

        except Exception as e:
            logger.exception(f"[MediaPipeThread] æ¨ç†å¼‚å¸¸: {e}")
            self.error_occurred.emit(str(e))
        finally:
            self.running = False
            if self.cap:
                self.cap.release()
            # é‡Šæ”¾MediaPipeèµ„æº
            if self.pose_landmarker:
                self.pose_landmarker.close()
            if self.hand_landmarker:
                self.hand_landmarker.close()
            if self.gesture_recognizer:
                self.gesture_recognizer.close()
            if self.face_landmarker:
                self.face_landmarker.close()

    def _process_frame(self, frame: np.ndarray, timestamp_ms: int) -> tuple:
        """å¤„ç†å•å¸§å›¾åƒ"""
        import mediapipe as mp

        result = MediaPipeResult()
        h, w = frame.shape[:2]

        # è½¬æ¢ä¸ºMediaPipeå›¾åƒæ ¼å¼
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)

        # å§¿æ€æ£€æµ‹
        if self.pose_landmarker:
            try:
                pose_result = self.pose_landmarker.detect_for_video(mp_image, timestamp_ms)
                if pose_result.pose_landmarks:
                    for pose_landmarks in pose_result.pose_landmarks:
                        pose = PoseResult()
                        pose.confidence = 1.0
                        for landmark in pose_landmarks:
                            pose.keypoints.append(Keypoint(
                                x=landmark.x,
                                y=landmark.y,
                                z=landmark.z,
                                visibility=landmark.visibility if hasattr(landmark, 'visibility') else 1.0
                            ))
                        result.poses.append(pose)
            except Exception as e:
                logger.error(f"å§¿æ€æ£€æµ‹å¤±è´¥: {e}")

        # æ‰‹åŠ¿è¯†åˆ«ï¼ˆå¦‚æœå¯ç”¨äº†æ‰‹åŠ¿è¯†åˆ«ï¼Œä½¿ç”¨GestureRecognizerï¼Œå®ƒåŒæ—¶è¿”å›å…³é”®ç‚¹å’Œæ‰‹åŠ¿ï¼‰
        if self.gesture_recognizer:
            try:
                gesture_result = self.gesture_recognizer.recognize_for_video(mp_image, timestamp_ms)
                if gesture_result.hand_landmarks:
                    for i, hand_landmarks in enumerate(gesture_result.hand_landmarks):
                        hand = HandResult()
                        for landmark in hand_landmarks:
                            hand.keypoints.append(Keypoint(
                                x=landmark.x,
                                y=landmark.y,
                                z=landmark.z
                            ))
                        # è·å–æ‰‹åŠ¿ç±»åˆ«
                        if gesture_result.gestures and i < len(gesture_result.gestures):
                            gesture = gesture_result.gestures[i]
                            if gesture:
                                # å–ç½®ä¿¡åº¦æœ€é«˜çš„æ‰‹åŠ¿
                                best_gesture = max(gesture, key=lambda g: g.score)
                                hand.gesture = best_gesture.category_name
                                hand.gesture_score = best_gesture.score
                        result.hands.append(hand)
            except Exception as e:
                logger.error(f"æ‰‹åŠ¿è¯†åˆ«å¤±è´¥: {e}")
        # ä»…æ‰‹éƒ¨æ£€æµ‹ï¼ˆä¸è¯†åˆ«æ‰‹åŠ¿ï¼‰
        elif self.hand_landmarker:
            try:
                hand_result = self.hand_landmarker.detect_for_video(mp_image, timestamp_ms)
                if hand_result.hand_landmarks:
                    for hand_landmarks in hand_result.hand_landmarks:
                        hand = HandResult()
                        for landmark in hand_landmarks:
                            hand.keypoints.append(Keypoint(
                                x=landmark.x,
                                y=landmark.y,
                                z=landmark.z
                            ))
                        result.hands.append(hand)
            except Exception as e:
                logger.error(f"æ‰‹éƒ¨æ£€æµ‹å¤±è´¥: {e}")

        # é¢éƒ¨æ£€æµ‹
        if self.face_landmarker:
            try:
                face_result = self.face_landmarker.detect_for_video(mp_image, timestamp_ms)
                if face_result.face_landmarks:
                    for face_landmarks in face_result.face_landmarks:
                        face_keypoints = []
                        for landmark in face_landmarks:
                            face_keypoints.append(Keypoint(
                                x=landmark.x,
                                y=landmark.y,
                                z=landmark.z
                            ))
                        result.faces.append(face_keypoints)
            except Exception as e:
                logger.error(f"é¢éƒ¨æ£€æµ‹å¤±è´¥: {e}")

        # ç»˜åˆ¶ç»“æœ
        display_frame = self._draw_results(frame, result)

        return display_frame, result

    def _draw_results(self, frame: np.ndarray, result: MediaPipeResult) -> np.ndarray:
        """ç»˜åˆ¶å…³é”®ç‚¹ç»“æœ"""
        display_frame = frame.copy()
        h, w = display_frame.shape[:2]

        # ç»˜åˆ¶å§¿æ€
        for pose in result.poses:
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            if pose.bbox:
                x1, y1, x2, y2 = map(int, pose.bbox)
                cv2.rectangle(display_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)

            # ç»˜åˆ¶å…³é”®ç‚¹
            for i, kp in enumerate(pose.keypoints):
                x, y = int(kp.x * w), int(kp.y * h)
                color = (0, 255, 0) if kp.visibility > 0.5 else (128, 128, 128)
                cv2.circle(display_frame, (x, y), 4, color, -1)
                cv2.putText(display_frame, str(i), (x+5, y),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.3, color, 1)

            # ç»˜åˆ¶éª¨éª¼è¿æ¥ (ç®€åŒ–çš„è¿æ¥)
            connections = [(0,1), (1,2), (2,3), (3,7), (0,4), (4,5), (5,6), (6,8),
                          (9,10), (11,12), (11,13), (13,15), (15,17), (12,14), (14,16), (16,18)]
            for start_idx, end_idx in connections:
                if len(pose.keypoints) > max(start_idx, end_idx):
                    kp1 = pose.keypoints[start_idx]
                    kp2 = pose.keypoints[end_idx]
                    if kp1.visibility > 0.5 and kp2.visibility > 0.5:
                        x1, y1 = int(kp1.x * w), int(kp1.y * h)
                        x2, y2 = int(kp2.x * w), int(kp2.y * h)
                        cv2.line(display_frame, (x1, y1), (x2, y2), (0, 255, 255), 2)

        # ç»˜åˆ¶æ‰‹éƒ¨
        for hand in result.hands:
            # ç»˜åˆ¶å…³é”®ç‚¹
            for kp in hand.keypoints:
                x, y = int(kp.x * w), int(kp.y * h)
                cv2.circle(display_frame, (x, y), 3, (255, 0, 0), -1)

            # ç»˜åˆ¶æ‰‹åŠ¿æ ‡ç­¾
            if hand.gesture:
                # è®¡ç®—æ‰‹éƒ¨ä¸­å¿ƒä½ç½®ç”¨äºæ˜¾ç¤ºæ ‡ç­¾
                if hand.keypoints:
                    center_x = int(sum(kp.x for kp in hand.keypoints) / len(hand.keypoints) * w)
                    center_y = int(sum(kp.y for kp in hand.keypoints) / len(hand.keypoints) * h)
                    # æ‰‹åŠ¿åç§°æ˜ å°„ï¼ˆè‹±æ–‡->ä¸­æ–‡/emojiï¼‰
                    gesture_map = {
                        'Thumb_Up': 'ğŸ‘ èµ',
                        'Thumb_Down': 'ğŸ‘ è¸©',
                        'Open_Palm': 'âœ‹ æ‰‹æŒ',
                        'Closed_Fist': 'âœŠ æ‹³å¤´',
                        'Victory': 'âœŒï¸ èƒœåˆ©',
                        'Pointing_Up': 'â˜ï¸ æŒ‡ä¸Š',
                        'ILoveYou': 'ğŸ¤Ÿ çˆ±ä½ ',
                        'None': 'æ— æ‰‹åŠ¿'
                    }
                    gesture_text = gesture_map.get(hand.gesture, hand.gesture)
                    # è®¡ç®—æ–‡å­—å¤§å°
                    temp_pil = Image.new('RGB', (1, 1))
                    temp_draw = ImageDraw.Draw(temp_pil)
                    try:
                        font = ImageFont.truetype("C:/Windows/Fonts/simhei.ttf", 20)
                    except:
                        font = ImageFont.load_default()
                    bbox = temp_draw.textbbox((0, 0), gesture_text, font=font)
                    text_w, text_h = bbox[2] - bbox[0], bbox[3] - bbox[1]

                    # ç»˜åˆ¶èƒŒæ™¯æ¡†
                    cv2.rectangle(display_frame,
                                 (center_x - text_w//2 - 5, center_y - text_h - 10),
                                 (center_x + text_w//2 + 5, center_y + 5),
                                 (255, 255, 0), -1)
                    # ä½¿ç”¨PILç»˜åˆ¶ä¸­æ–‡æ–‡å­—
                    display_frame = self._draw_chinese_text(
                        display_frame, gesture_text,
                        (center_x - text_w//2, center_y - text_h - 5),
                        20, (0, 0, 0)
                    )

        # ç»˜åˆ¶é¢éƒ¨
        for face in result.faces:
            for kp in face:
                x, y = int(kp.x * w), int(kp.y * h)
                cv2.circle(display_frame, (x, y), 1, (0, 0, 255), -1)

        # æ˜¾ç¤ºæ¨¡å¼ä¿¡æ¯
        display_frame = self._draw_chinese_text(display_frame, "MediaPipe Tasks API", (10, 30), 20, (0, 255, 255))

        return display_frame

    def _draw_chinese_text(self, img: np.ndarray, text: str, position: tuple, font_size: int, color: tuple) -> np.ndarray:
        """ä½¿ç”¨PILç»˜åˆ¶ä¸­æ–‡æ–‡å­—"""
        # è½¬æ¢OpenCVå›¾åƒä¸ºPILå›¾åƒ
        img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(img_pil)

        # å°è¯•åŠ è½½ä¸­æ–‡å­—ä½“
        font = None
        font_paths = [
            "C:/Windows/Fonts/simhei.ttf",  # é»‘ä½“
            "C:/Windows/Fonts/simsun.ttc",  # å®‹ä½“
            "C:/Windows/Fonts/msyh.ttc",    # å¾®è½¯é›…é»‘
            "C:/Windows/Fonts/simkai.ttf",  # æ¥·ä½“
        ]

        for font_path in font_paths:
            try:
                font = ImageFont.truetype(font_path, font_size)
                break
            except:
                continue

        if font is None:
            # å¦‚æœæ‰¾ä¸åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“
            font = ImageFont.load_default()

        # ç»˜åˆ¶æ–‡å­—
        draw.text(position, text, font=font, fill=color[::-1])  # BGR to RGB

        # è½¬æ¢å›OpenCVå›¾åƒ
        return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)

    def _run_camera_inference(self):
        """æ‘„åƒå¤´æ¨ç†"""
        camera_id = self.source_config.get('camera_id', 0)
        self.cap = cv2.VideoCapture(camera_id)

        if not self.cap.isOpened():
            raise Exception(f"æ— æ³•æ‰“å¼€æ‘„åƒå¤´ {camera_id}")

        frame_count = 0
        start_time = datetime.now()
        start_timestamp_ms = int(start_time.timestamp() * 1000)

        while self.running:
            ret, frame = self.cap.read()
            if not ret:
                continue

            # è®¡ç®—å½“å‰æ—¶é—´æˆ³
            current_timestamp_ms = int(datetime.now().timestamp() * 1000) - start_timestamp_ms

            display_frame, result = self._process_frame(frame, current_timestamp_ms)

            # è®¡ç®—FPS
            frame_count += 1
            elapsed = (datetime.now() - start_time).total_seconds()
            if elapsed > 0:
                result.fps = frame_count / elapsed
                result.inference_time = elapsed / frame_count

            self.frame_ready.emit(display_frame, result)
            self.fps_updated.emit(result.fps)

    def _run_file_inference(self):
        """æ–‡ä»¶æ¨ç†"""
        file_path = self.source_config.get('file_path', '')

        if os.path.isfile(file_path):
            self.cap = cv2.VideoCapture(file_path)
            frame_count = 0
            start_time = datetime.now()
            start_timestamp_ms = int(start_time.timestamp() * 1000)

            while self.running and self.cap.isOpened():
                ret, frame = self.cap.read()
                if not ret:
                    break

                # è®¡ç®—å½“å‰æ—¶é—´æˆ³
                current_timestamp_ms = int(datetime.now().timestamp() * 1000) - start_timestamp_ms

                display_frame, result = self._process_frame(frame, current_timestamp_ms)

                frame_count += 1
                elapsed = (datetime.now() - start_time).total_seconds()
                if elapsed > 0:
                    result.fps = frame_count / elapsed

                self.frame_ready.emit(display_frame, result)
                self.fps_updated.emit(result.fps)

    def stop(self):
        """åœæ­¢æ¨ç†"""
        self.running = False
        self.wait(1000)


# ============== YOLO æ¨ç†çº¿ç¨‹ ==============
class InferenceThread(QThread):
    """YOLOæ¨ç†çº¿ç¨‹"""
    
    frame_ready = pyqtSignal(np.ndarray, InferenceResult)
    fps_updated = pyqtSignal(float)
    error_occurred = pyqtSignal(str)
    
    def __init__(self, inference_engine: YOLOInference, source_type: str, source_config: dict):
        super().__init__()
        self.inference_engine = inference_engine
        self.source_type = source_type
        self.source_config = source_config
        self.running = False
        self.cap = None
    
    def run(self):
        """è¿è¡Œæ¨ç†"""
        logger.info(f"[InferenceThread] çº¿ç¨‹å¼€å§‹è¿è¡Œï¼Œæºç±»å‹: {self.source_type}")
        self.running = True
        
        try:
            if self.source_type == "camera":
                self._run_camera_inference()
            elif self.source_type == "file":
                self._run_file_inference()
            elif self.source_type == "http":
                self._run_http_inference()
            elif self.source_type == "mqtt":
                self._run_mqtt_inference()
        except Exception as e:
            logger.exception(f"[InferenceThread] æ¨ç†çº¿ç¨‹å¼‚å¸¸: {e}")
            self.error_occurred.emit(str(e))
        finally:
            self.running = False
            if self.cap:
                self.cap.release()
    
    def _run_camera_inference(self):
        """æ‘„åƒå¤´æ¨ç†"""
        camera_id = self.source_config.get('camera_id', 0)
        self.cap = cv2.VideoCapture(camera_id)
        
        if not self.cap.isOpened():
            raise Exception(f"æ— æ³•æ‰“å¼€æ‘„åƒå¤´ {camera_id}")
        
        while self.running:
            ret, frame = self.cap.read()
            if not ret:
                continue
            
            if self.inference_engine is None:
                break
            
            result = self.inference_engine.infer(frame)
            display_frame = self.inference_engine.draw_results(frame, result)
            
            self.frame_ready.emit(display_frame, result)
            self.fps_updated.emit(result.fps)
    
    def _run_file_inference(self):
        """æ–‡ä»¶æ¨ç†"""
        file_path = self.source_config.get('file_path', '')
        
        if os.path.isfile(file_path):
            self.cap = cv2.VideoCapture(file_path)
            while self.running and self.cap.isOpened():
                ret, frame = self.cap.read()
                if not ret:
                    break
                
                result = self.inference_engine.infer(frame)
                display_frame = self.inference_engine.draw_results(frame, result)
                
                self.frame_ready.emit(display_frame, result)
                self.fps_updated.emit(result.fps)
    
    def _run_http_inference(self):
        """HTTPæµæ¨ç†"""
        url = self.source_config.get('http_url', '')
        self.cap = cv2.VideoCapture(url)
        
        if not self.cap.isOpened():
            raise Exception(f"æ— æ³•æ‰“å¼€è§†é¢‘æµ {url}")
        
        while self.running:
            ret, frame = self.cap.read()
            if not ret:
                continue
            
            result = self.inference_engine.infer(frame)
            display_frame = self.inference_engine.draw_results(frame, result)
            
            self.frame_ready.emit(display_frame, result)
            self.fps_updated.emit(result.fps)
    
    def _run_mqtt_inference(self):
        """MQTTå›¾åƒæ¨ç†"""
        pass
    
    def stop(self):
        """åœæ­¢æ¨ç†"""
        self.running = False
        self.wait(1000)


# ============== YOLO ä¸“æ  ==============
class YOLOPanel(QWidget):
    """YOLOç›®æ ‡æ£€æµ‹ä¸“æ """
    
    # ä¿¡å·
    model_load_requested = pyqtSignal(str, int)  # æ¨¡å‹è·¯å¾„, ä»»åŠ¡ç±»å‹
    inference_start_requested = pyqtSignal()
    inference_stop_requested = pyqtSignal()
    
    def __init__(self, config_manager: ConfigManager):
        super().__init__()
        
        self.config_manager = config_manager
        self.config = config_manager.get_config()
        
        self.inference_engine: Optional[YOLOInference] = None
        self.inference_thread: Optional[InferenceThread] = None
        self.is_running = False
        self.current_fps = 0.0
        
        self._init_ui()
        self._load_config()
    
    def _init_ui(self):
        """åˆå§‹åŒ–UI"""
        layout = QVBoxLayout(self)
        layout.setContentsMargins(5, 5, 5, 5)
        layout.setSpacing(10)
        
        # æ ‡é¢˜
        title_label = QLabel("ğŸ¯ YOLO ç›®æ ‡æ£€æµ‹")
        title_label.setStyleSheet("font-size: 16px; font-weight: bold; color: #4caf50;")
        title_label.setAlignment(Qt.AlignmentFlag.AlignCenter)
        layout.addWidget(title_label)
        
        # åˆ†å‰²å™¨
        splitter = QSplitter(Qt.Orientation.Horizontal)
        layout.addWidget(splitter)
        
        # å·¦ä¾§æ§åˆ¶é¢æ¿
        left_panel = self._create_left_panel()
        splitter.addWidget(left_panel)
        
        # å³ä¾§è§†é¢‘æ˜¾ç¤ºé¢æ¿
        right_panel = self._create_right_panel()
        splitter.addWidget(right_panel)
        
        # è®¾ç½®åˆ†å‰²æ¯”ä¾‹
        splitter.setSizes([350, 850])
    
    def _create_left_panel(self) -> QWidget:
        """åˆ›å»ºå·¦ä¾§æ§åˆ¶é¢æ¿"""
        panel = QWidget()
        layout = QVBoxLayout(panel)
        layout.setSpacing(10)
        
        # æ¨¡å‹è®¾ç½®ç»„
        model_group = QGroupBox("æ¨¡å‹è®¾ç½®")
        model_layout = QVBoxLayout(model_group)
        
        # æ¨¡å‹é€‰æ‹©æ–¹å¼
        model_select_layout = QHBoxLayout()
        model_select_layout.addWidget(QLabel("æ¨¡å‹æ¥æº:"))
        
        self.model_source_combo = QComboBox()
        self.model_source_combo.addItems(["å®˜æ–¹é¢„è®­ç»ƒæ¨¡å‹", "è‡ªå®šä¹‰æ¨¡å‹"])
        self.model_source_combo.currentIndexChanged.connect(self._on_model_source_changed)
        model_select_layout.addWidget(self.model_source_combo)
        
        model_layout.addLayout(model_select_layout)
        
        # å®˜æ–¹æ¨¡å‹é€‰æ‹©
        self.official_model_widget = QWidget()
        official_layout = QHBoxLayout(self.official_model_widget)
        official_layout.setContentsMargins(0, 0, 0, 0)
        official_layout.addWidget(QLabel("ä»»åŠ¡ç±»å‹:"))
        self.task_type_combo = QComboBox()
        self.task_type_combo.addItems([
            "ç›®æ ‡æ£€æµ‹ (Detection)",
            "å®ä¾‹åˆ†å‰² (Segmentation)",
            "å§¿æ€ä¼°è®¡ (Pose)",
            "å®šå‘æ£€æµ‹ (OBB)",
            "å›¾åƒåˆ†ç±» (Classification)"
        ])
        self.task_type_combo.currentIndexChanged.connect(self._on_task_type_changed)
        official_layout.addWidget(self.task_type_combo)
        
        official_layout.addWidget(QLabel("æ¨¡å‹:"))
        self.model_size_combo = QComboBox()
        self._update_model_combo_items()
        self.model_size_combo.currentIndexChanged.connect(self._check_official_model_status)
        official_layout.addWidget(self.model_size_combo)
        
        self.download_model_btn = QPushButton("ä¸‹è½½æ¨¡å‹")
        self.download_model_btn.setStyleSheet("background-color: #ff9800;")
        self.download_model_btn.clicked.connect(self._download_official_model)
        official_layout.addWidget(self.download_model_btn)
        
        model_layout.addWidget(self.official_model_widget)
        
        # è‡ªå®šä¹‰æ¨¡å‹è·¯å¾„
        self.custom_model_widget = QWidget()
        custom_layout = QVBoxLayout(self.custom_model_widget)
        custom_layout.setContentsMargins(0, 0, 0, 0)
        custom_layout.setSpacing(10)
        
        custom_task_layout = QHBoxLayout()
        custom_task_layout.addWidget(QLabel("æ¨¡å‹ç±»åˆ«:"))
        self.custom_task_type_combo = QComboBox()
        self.custom_task_type_combo.addItems([
            "ç›®æ ‡æ£€æµ‹ (Detection)",
            "å®ä¾‹åˆ†å‰² (Segmentation)",
            "å§¿æ€ä¼°è®¡ (Pose)",
            "å®šå‘æ£€æµ‹ (OBB)",
            "å›¾åƒåˆ†ç±» (Classification)"
        ])
        custom_task_layout.addWidget(self.custom_task_type_combo)
        custom_layout.addLayout(custom_task_layout)
        
        path_layout = QHBoxLayout()
        self.model_path_input = QLineEdit()
        self.model_path_input.setPlaceholderText("é€‰æ‹©æ¨¡å‹æ–‡ä»¶...")
        path_layout.addWidget(self.model_path_input)
        
        self.browse_model_btn = QPushButton("æµè§ˆ")
        self.browse_model_btn.clicked.connect(self._browse_model)
        path_layout.addWidget(self.browse_model_btn)
        custom_layout.addLayout(path_layout)
        
        model_layout.addWidget(self.custom_model_widget)
        self.custom_model_widget.setVisible(False)
        
        self.model_type_combo = QComboBox()
        self.model_type_combo.addItems(["ç›®æ ‡æ£€æµ‹", "å®ä¾‹åˆ†å‰²", "å§¿æ€ä¼°è®¡", "å®šå‘æ£€æµ‹", "å›¾åƒåˆ†ç±»"])
        self.model_type_combo.setVisible(False)
        self.task_type_combo.currentIndexChanged.connect(self._sync_model_type)
        
        self.load_model_btn = QPushButton("åŠ è½½æ¨¡å‹")
        self.load_model_btn.setStyleSheet("background-color: #2196f3;")
        self.load_model_btn.clicked.connect(self._load_model)
        model_layout.addWidget(self.load_model_btn)
        
        self.model_status_label = QLabel("æ¨¡å‹çŠ¶æ€: æœªåŠ è½½")
        model_layout.addWidget(self.model_status_label)
        
        layout.addWidget(model_group)
        
        # æ¨ç†æºè®¾ç½®ç»„
        source_group = QGroupBox("æ¨ç†æºè®¾ç½®")
        source_layout = QVBoxLayout(source_group)
        
        self.source_type_combo = QComboBox()
        self.source_type_combo.addItems(["æ‘„åƒå¤´", "æœ¬åœ°æ–‡ä»¶", "HTTPæ¨æµ", "MQTTå›¾åƒ"])
        self.source_type_combo.currentIndexChanged.connect(self._on_source_type_changed)
        source_layout.addWidget(self.source_type_combo)
        
        self.source_config_widget = QWidget()
        self.source_config_layout = QVBoxLayout(self.source_config_widget)
        self.source_config_layout.setContentsMargins(0, 0, 0, 0)
        source_layout.addWidget(self.source_config_widget)
        
        # æ‘„åƒå¤´é…ç½®
        self.camera_config = QWidget()
        camera_layout = QHBoxLayout(self.camera_config)
        camera_layout.setContentsMargins(0, 0, 0, 0)
        camera_layout.addWidget(QLabel("æ‘„åƒå¤´ID:"))
        self.camera_id_spin = QSpinBox()
        self.camera_id_spin.setRange(0, 10)
        camera_layout.addWidget(self.camera_id_spin)
        camera_layout.addStretch()
        self.source_config_layout.addWidget(self.camera_config)
        
        # æ–‡ä»¶é…ç½®
        self.file_config = QWidget()
        file_layout = QHBoxLayout(self.file_config)
        file_layout.setContentsMargins(0, 0, 0, 0)
        self.file_path_input = QLineEdit()
        self.file_path_input.setPlaceholderText("é€‰æ‹©æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹...")
        file_layout.addWidget(self.file_path_input)
        self.browse_file_btn = QPushButton("æµè§ˆ")
        self.browse_file_btn.clicked.connect(self._browse_file)
        file_layout.addWidget(self.browse_file_btn)
        self.source_config_layout.addWidget(self.file_config)
        self.file_config.setVisible(False)
        
        # HTTPé…ç½®
        self.http_config = QWidget()
        http_layout = QHBoxLayout(self.http_config)
        http_layout.setContentsMargins(0, 0, 0, 0)
        http_layout.addWidget(QLabel("URL:"))
        self.http_url_input = QLineEdit()
        self.http_url_input.setPlaceholderText("rtsp://... æˆ– http://...")
        http_layout.addWidget(self.http_url_input)
        self.source_config_layout.addWidget(self.http_config)
        self.http_config.setVisible(False)
        
        # MQTTé…ç½®
        self.mqtt_config = QWidget()
        mqtt_layout = QHBoxLayout(self.mqtt_config)
        mqtt_layout.setContentsMargins(0, 0, 0, 0)
        mqtt_layout.addWidget(QLabel("ä¸»é¢˜:"))
        self.mqtt_topic_input = QLineEdit()
        self.mqtt_topic_input.setPlaceholderText("inference/image")
        mqtt_layout.addWidget(self.mqtt_topic_input)
        self.source_config_layout.addWidget(self.mqtt_config)
        self.mqtt_config.setVisible(False)
        
        layout.addWidget(source_group)
        
        # æ¨ç†å‚æ•°ç»„
        params_group = QGroupBox("æ¨ç†å‚æ•°")
        params_layout = QFormLayout(params_group)
        
        self.conf_threshold_spin = QDoubleSpinBox()
        self.conf_threshold_spin.setRange(0.0, 1.0)
        self.conf_threshold_spin.setSingleStep(0.05)
        self.conf_threshold_spin.setValue(0.5)
        params_layout.addRow("ç½®ä¿¡åº¦é˜ˆå€¼:", self.conf_threshold_spin)
        
        self.iou_threshold_spin = QDoubleSpinBox()
        self.iou_threshold_spin.setRange(0.0, 1.0)
        self.iou_threshold_spin.setSingleStep(0.05)
        self.iou_threshold_spin.setValue(0.45)
        params_layout.addRow("IOUé˜ˆå€¼:", self.iou_threshold_spin)
        
        self.img_size_spin = QSpinBox()
        self.img_size_spin.setRange(32, 1280)
        self.img_size_spin.setSingleStep(32)
        self.img_size_spin.setValue(640)
        params_layout.addRow("å›¾åƒå°ºå¯¸:", self.img_size_spin)
        
        self.half_precision_check = QCheckBox("ä½¿ç”¨åŠç²¾åº¦(FP16)")
        params_layout.addRow("åŠç²¾åº¦:", self.half_precision_check)
        
        layout.addWidget(params_group)
        
        # æ§åˆ¶æŒ‰é’®
        btn_layout = QHBoxLayout()
        
        self.start_btn = QPushButton("å¼€å§‹æ¨ç† (F5)")
        self.start_btn.setStyleSheet("background-color: #4caf50; font-size: 14px; padding: 10px;")
        self.start_btn.clicked.connect(self._start_inference)
        btn_layout.addWidget(self.start_btn)
        
        self.stop_btn = QPushButton("åœæ­¢")
        self.stop_btn.setStyleSheet("background-color: #f44336; font-size: 14px; padding: 10px;")
        self.stop_btn.setEnabled(False)
        self.stop_btn.clicked.connect(self._stop_inference)
        btn_layout.addWidget(self.stop_btn)
        
        layout.addLayout(btn_layout)
        
        # æˆªå›¾æŒ‰é’®
        self.screenshot_btn = QPushButton("æˆªå›¾ (F6)")
        self.screenshot_btn.clicked.connect(self.take_screenshot)
        layout.addWidget(self.screenshot_btn)
        
        # æ¨ç†ç»Ÿè®¡
        stats_group = QGroupBox("æ¨ç†ç»Ÿè®¡")
        stats_layout = QVBoxLayout(stats_group)
        
        self.fps_label = QLabel("FPS: 0")
        stats_layout.addWidget(self.fps_label)
        
        self.inference_time_label = QLabel("æ¨ç†æ—¶é—´: 0 ms")
        stats_layout.addWidget(self.inference_time_label)
        
        self.detection_count_label = QLabel("æ£€æµ‹æ•°é‡: 0")
        stats_layout.addWidget(self.detection_count_label)
        
        layout.addWidget(stats_group)
        
        layout.addStretch()
        return panel
    
    def _create_right_panel(self) -> QWidget:
        """åˆ›å»ºå³ä¾§æ˜¾ç¤ºé¢æ¿"""
        panel = QWidget()
        layout = QVBoxLayout(panel)
        layout.setSpacing(10)
        
        # è§†é¢‘æ˜¾ç¤ºæ ‡ç­¾
        self.video_label = QLabel()
        self.video_label.setMinimumSize(640, 480)
        self.video_label.setAlignment(Qt.AlignmentFlag.AlignCenter)
        self.video_label.setStyleSheet("background-color: #1a1a1a; border: 2px solid #555;")
        self.video_label.setText("ç­‰å¾…å¼€å§‹æ¨ç†...")
        layout.addWidget(self.video_label)
        
        # æ£€æµ‹ç»“æœTab
        self.result_tabs = QTabWidget()
        
        # æ£€æµ‹åˆ—è¡¨Tab
        self.detection_list = QTableWidget()
        self.detection_list.setColumnCount(4)
        self.detection_list.setHorizontalHeaderLabels(["ç±»åˆ«", "ç½®ä¿¡åº¦", "ä½ç½®", "æ“ä½œ"])
        self.detection_list.horizontalHeader().setStretchLastSection(True)
        self.result_tabs.addTab(self.detection_list, "æ£€æµ‹ç»“æœ")
        
        # æ—¥å¿—Tab
        self.log_text = QTextEdit()
        self.log_text.setReadOnly(True)
        self.result_tabs.addTab(self.log_text, "æ—¥å¿—")
        
        layout.addWidget(self.result_tabs)
        
        return panel
    
    def _load_config(self):
        """åŠ è½½é…ç½®"""
        self.model_path_input.setText(self.config.inference.model_path)
        
        model_type_index = {
            "detection": 0, "segmentation": 1, "pose": 2, "classification": 3
        }.get(self.config.inference.model_type, 0)
        self.model_type_combo.setCurrentIndex(model_type_index)
        
        source_type_index = {
            "camera": 0, "file": 1, "http": 2, "mqtt": 3
        }.get(self.config.source.source_type, 0)
        self.source_type_combo.setCurrentIndex(source_type_index)
        
        self.camera_id_spin.setValue(self.config.source.camera_id)
        self.file_path_input.setText(self.config.source.file_path)
        self.http_url_input.setText(self.config.source.http_url)
        self.mqtt_topic_input.setText(self.config.source.mqtt_topic)
        
        self.conf_threshold_spin.setValue(self.config.inference.conf_threshold)
        self.iou_threshold_spin.setValue(self.config.inference.iou_threshold)
        self.img_size_spin.setValue(self.config.inference.img_size)
        self.half_precision_check.setChecked(self.config.inference.half_precision)
    
    def _on_source_type_changed(self, index: int):
        """æºç±»å‹æ”¹å˜"""
        self.camera_config.setVisible(index == 0)
        self.file_config.setVisible(index == 1)
        self.http_config.setVisible(index == 2)
        self.mqtt_config.setVisible(index == 3)
    
    def _on_model_source_changed(self, index: int):
        """æ¨¡å‹æ¥æºæ”¹å˜"""
        is_official = (index == 0)
        self.official_model_widget.setVisible(is_official)
        self.custom_model_widget.setVisible(not is_official)
        if is_official:
            self._check_official_model_status()
    
    def _on_task_type_changed(self):
        """ä»»åŠ¡ç±»å‹æ”¹å˜"""
        self._update_model_combo_items()
        self._check_official_model_status()
    
    def _update_model_combo_items(self):
        """æ›´æ–°æ¨¡å‹ä¸‹æ‹‰æ¡†é€‰é¡¹"""
        task_type = self.task_type_combo.currentIndex()
        suffixes = ["", "-seg", "-pose", "-obb", "-cls"]
        suffix = suffixes[task_type] if task_type < len(suffixes) else ""
        
        models = [
            (f"yolo26n{suffix}", "Nano - å¿«é€Ÿ"),
            (f"yolo26s{suffix}", "Small - å¹³è¡¡"),
            (f"yolo26m{suffix}", "Medium - ç²¾ç¡®"),
            (f"yolo26l{suffix}", "Large - é«˜ç²¾åº¦"),
            (f"yolo26x{suffix}", "XLarge - æœ€é«˜ç²¾åº¦")
        ]
        
        current_index = self.model_size_combo.currentIndex()
        self.model_size_combo.clear()
        for model_name, desc in models:
            self.model_size_combo.addItem(f"{model_name} ({desc})")
        
        if current_index >= 0 and current_index < self.model_size_combo.count():
            self.model_size_combo.setCurrentIndex(current_index)
    
    def _sync_model_type(self):
        """åŒæ­¥ä»»åŠ¡ç±»å‹å’Œæ¨¡å‹ç±»å‹"""
        task_index = self.task_type_combo.currentIndex()
        if task_index >= 0 and task_index < self.model_type_combo.count():
            self.model_type_combo.setCurrentIndex(task_index)
    
    def _get_current_model_name(self) -> str:
        """è·å–å½“å‰é€‰ä¸­çš„å®Œæ•´æ¨¡å‹åç§°"""
        full_text = self.model_size_combo.currentText()
        model_name = full_text.split()[0] if full_text else "yolo26n"
        return model_name
    
    def _check_official_model_status(self):
        """æ£€æŸ¥å®˜æ–¹æ¨¡å‹çŠ¶æ€"""
        model_name = self._get_current_model_name()
        model_path = self._get_official_model_path(model_name)
        
        if model_path.exists():
            self.download_model_btn.setText("å·²ä¸‹è½½")
            self.download_model_btn.setStyleSheet("background-color: #4caf50;")
            self.download_model_btn.setEnabled(False)
            self.model_path_input.setText(str(model_path))
        else:
            self.download_model_btn.setText("ä¸‹è½½æ¨¡å‹")
            self.download_model_btn.setStyleSheet("background-color: #ff9800;")
            self.download_model_btn.setEnabled(True)
    
    def _get_official_model_path(self, model_name: str) -> Path:
        """è·å–å®˜æ–¹æ¨¡å‹ä¿å­˜è·¯å¾„"""
        models_dir = Path(__file__).parent.parent.parent / "models"
        models_dir.mkdir(exist_ok=True)
        return models_dir / f"{model_name}.pt"
    
    def _download_official_model(self):
        """ä¸‹è½½å®˜æ–¹æ¨¡å‹"""
        # ç®€åŒ–ç‰ˆï¼Œå®é™…å®ç°éœ€è¦ä¸‹è½½é€»è¾‘
        QMessageBox.information(self, "æç¤º", "æ¨¡å‹ä¸‹è½½åŠŸèƒ½éœ€è¦ç½‘ç»œè¿æ¥")
    
    def _browse_model(self):
        """æµè§ˆæ¨¡å‹æ–‡ä»¶"""
        file_path, _ = QFileDialog.getOpenFileName(
            self, "é€‰æ‹©æ¨¡å‹æ–‡ä»¶", "", "æ¨¡å‹æ–‡ä»¶ (*.pt *.pth *.onnx);;æ‰€æœ‰æ–‡ä»¶ (*.*)"
        )
        if file_path:
            self.model_path_input.setText(file_path)
    
    def _browse_file(self):
        """æµè§ˆæ–‡ä»¶"""
        file_path, _ = QFileDialog.getOpenFileName(
            self, "é€‰æ‹©è§†é¢‘æˆ–å›¾åƒ", "", "è§†é¢‘/å›¾åƒ (*.mp4 *.avi *.jpg *.jpeg *.png);;æ‰€æœ‰æ–‡ä»¶ (*.*)"
        )
        if file_path:
            self.file_path_input.setText(file_path)
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹"""
        if self.model_source_combo.currentIndex() == 0:
            model_name = self._get_current_model_name()
            model_path = str(self._get_official_model_path(model_name))
            task_type = self.task_type_combo.currentIndex()
            
            if not Path(model_path).exists():
                reply = QMessageBox.question(
                    self, "æ¨¡å‹æœªä¸‹è½½", f"å®˜æ–¹æ¨¡å‹ {model_name} å°šæœªä¸‹è½½ã€‚\n\næ˜¯å¦ç«‹å³ä¸‹è½½?",
                    QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.No
                )
                if reply == QMessageBox.StandardButton.Yes:
                    self._download_official_model()
                return
        else:
            model_path = self.model_path_input.text().strip()
            if not model_path:
                QMessageBox.warning(self, "è­¦å‘Š", "è¯·é€‰æ‹©æ¨¡å‹æ–‡ä»¶")
                return
            task_type = self.custom_task_type_combo.currentIndex()
        
        self.model_load_requested.emit(model_path, task_type)
    
    def on_model_loaded(self, success: bool):
        """æ¨¡å‹åŠ è½½å›è°ƒ"""
        if success:
            self.model_status_label.setText("æ¨¡å‹çŠ¶æ€: å·²åŠ è½½")
            self.model_status_label.setStyleSheet("color: #4caf50;")
            self._log("æ¨¡å‹åŠ è½½æˆåŠŸ")
        else:
            self.model_status_label.setText("æ¨¡å‹çŠ¶æ€: åŠ è½½å¤±è´¥")
            self.model_status_label.setStyleSheet("color: #f44336;")
            self._log("æ¨¡å‹åŠ è½½å¤±è´¥")
    
    def set_inference_engine(self, engine: YOLOInference):
        """è®¾ç½®æ¨ç†å¼•æ“"""
        self.inference_engine = engine
    
    def _start_inference(self):
        """å¼€å§‹æ¨ç†"""
        if self.inference_engine is None:
            QMessageBox.warning(self, "è­¦å‘Š", "è¯·å…ˆåŠ è½½æ¨¡å‹")
            return
        
        self._save_config()
        
        source_type = ["camera", "file", "http", "mqtt"][self.source_type_combo.currentIndex()]
        source_config = {}
        
        if source_type == "camera":
            source_config['camera_id'] = self.camera_id_spin.value()
        elif source_type == "file":
            source_config['file_path'] = self.file_path_input.text()
        elif source_type == "http":
            source_config['http_url'] = self.http_url_input.text()
        elif source_type == "mqtt":
            source_config['mqtt_topic'] = self.mqtt_topic_input.text()
        
        self.inference_thread = InferenceThread(
            self.inference_engine, source_type, source_config
        )
        self.inference_thread.frame_ready.connect(self._on_frame_ready)
        self.inference_thread.fps_updated.connect(self._on_fps_updated)
        self.inference_thread.error_occurred.connect(self._on_inference_error)
        self.inference_thread.start()
        
        self.is_running = True
        self.start_btn.setEnabled(False)
        self.stop_btn.setEnabled(True)
        self._log(f"å¼€å§‹æ¨ç†: {source_type}")
    
    def stop_inference(self):
        """å…¬å…±æ–¹æ³•ï¼šåœæ­¢æ¨ç†"""
        self._stop_inference()
    
    def _stop_inference(self):
        """åœæ­¢æ¨ç†"""
        if self.inference_thread:
            self.inference_thread.stop()
            self.inference_thread = None
        
        self.is_running = False
        self.start_btn.setEnabled(True)
        self.stop_btn.setEnabled(False)
        self.video_label.setText("æ¨ç†å·²åœæ­¢")
        self._log("æ¨ç†å·²åœæ­¢")
    
    def _on_frame_ready(self, frame: np.ndarray, result: InferenceResult):
        """å¸§å°±ç»ªå›è°ƒ"""
        try:
            if frame is None or frame.size == 0:
                return
            
            if not frame.flags['C_CONTIGUOUS']:
                frame = np.ascontiguousarray(frame)
            
            rgb_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            if not rgb_image.flags['C_CONTIGUOUS']:
                rgb_image = np.ascontiguousarray(rgb_image)
            
            h, w, ch = rgb_image.shape
            bytes_per_line = ch * w
            
            qt_image = QImage(rgb_image.copy().data, w, h, bytes_per_line, QImage.Format.Format_RGB888)
            pixmap = QPixmap.fromImage(qt_image)
            
            scaled_pixmap = pixmap.scaled(
                self.video_label.size(),
                Qt.AspectRatioMode.KeepAspectRatio,
                Qt.TransformationMode.SmoothTransformation
            )
            
            self.video_label.setPixmap(scaled_pixmap)
            self._update_detection_list(result)
            
        except Exception as e:
            logger.exception(f"å¤„ç†å¸§æ—¶å‡ºé”™: {e}")
    
    def _on_fps_updated(self, fps: float):
        """FPSæ›´æ–°å›è°ƒ"""
        self.current_fps = fps
        self.fps_label.setText(f"FPS: {fps:.1f}")
    
    def _on_inference_error(self, error: str):
        """æ¨ç†é”™è¯¯å›è°ƒ"""
        self._log(f"æ¨ç†é”™è¯¯: {error}")
        QMessageBox.critical(self, "æ¨ç†é”™è¯¯", error)
        self._stop_inference()
    
    def _update_detection_list(self, result: InferenceResult):
        """æ›´æ–°æ£€æµ‹åˆ—è¡¨"""
        if result.classifications:
            self.detection_list.setRowCount(len(result.classifications))
            for i, cls in enumerate(result.classifications):
                self.detection_list.setItem(i, 0, QTableWidgetItem(cls.class_name))
                self.detection_list.setItem(i, 1, QTableWidgetItem(f"{cls.confidence:.2f}"))
                top5_text = ", ".join([f"{name}:{conf:.2f}" for name, conf in cls.top5[:3]])
                self.detection_list.setItem(i, 2, QTableWidgetItem(top5_text))
            self.detection_count_label.setText(f"åˆ†ç±»ç»“æœ: {len(result.classifications)}")
        elif result.segmentations:
            self.detection_list.setRowCount(len(result.segmentations))
            for i, seg in enumerate(result.segmentations):
                self.detection_list.setItem(i, 0, QTableWidgetItem(seg.class_name))
                self.detection_list.setItem(i, 1, QTableWidgetItem(f"{seg.confidence:.2f}"))
                bbox_text = f"[{seg.bbox[0]:.0f}, {seg.bbox[1]:.0f}, {seg.bbox[2]:.0f}, {seg.bbox[3]:.0f}]"
                self.detection_list.setItem(i, 2, QTableWidgetItem(bbox_text))
            self.detection_count_label.setText(f"åˆ†å‰²æ•°é‡: {len(result.segmentations)}")
        elif result.keypoints:
            self.detection_list.setRowCount(len(result.keypoints))
            for i, kpt in enumerate(result.keypoints):
                self.detection_list.setItem(i, 0, QTableWidgetItem(f"å§¿æ€ {i+1}"))
                self.detection_list.setItem(i, 1, QTableWidgetItem(f"{kpt.confidence:.2f}"))
                bbox_text = f"[{kpt.bbox[0]:.0f}, {kpt.bbox[1]:.0f}, {kpt.bbox[2]:.0f}, {kpt.bbox[3]:.0f}]"
                self.detection_list.setItem(i, 2, QTableWidgetItem(bbox_text))
            self.detection_count_label.setText(f"å§¿æ€æ•°é‡: {len(result.keypoints)}")
        else:
            self.detection_list.setRowCount(len(result.detections))
            for i, det in enumerate(result.detections):
                self.detection_list.setItem(i, 0, QTableWidgetItem(det.class_name))
                self.detection_list.setItem(i, 1, QTableWidgetItem(f"{det.confidence:.2f}"))
                bbox_text = f"[{det.bbox[0]:.0f}, {det.bbox[1]:.0f}, {det.bbox[2]:.0f}, {det.bbox[3]:.0f}]"
                self.detection_list.setItem(i, 2, QTableWidgetItem(bbox_text))
            self.detection_count_label.setText(f"æ£€æµ‹æ•°é‡: {len(result.detections)}")
        
        self.inference_time_label.setText(f"æ¨ç†æ—¶é—´: {result.inference_time*1000:.1f} ms")
    
    def _save_config(self):
        """ä¿å­˜é…ç½®"""
        self.config.inference.model_path = self.model_path_input.text()
        self.config.inference.model_type = ["detection", "segmentation", "pose", "obb", "classification"][
            self.task_type_combo.currentIndex()
        ]
        self.config.source.source_type = ["camera", "file", "http", "mqtt"][
            self.source_type_combo.currentIndex()
        ]
        self.config.source.camera_id = self.camera_id_spin.value()
        self.config.source.file_path = self.file_path_input.text()
        self.config.source.http_url = self.http_url_input.text()
        self.config.source.mqtt_topic = self.mqtt_topic_input.text()
        self.config.inference.conf_threshold = self.conf_threshold_spin.value()
        self.config.inference.iou_threshold = self.iou_threshold_spin.value()
        self.config.inference.img_size = self.img_size_spin.value()
        self.config.inference.half_precision = self.half_precision_check.isChecked()
        self.config_manager.save()
    
    def take_screenshot(self):
        """æˆªå›¾"""
        if self.video_label.pixmap():
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"screenshot_{timestamp}.png"
            screenshot_dir = Path("screenshots")
            screenshot_dir.mkdir(exist_ok=True)
            filepath = screenshot_dir / filename
            self.video_label.pixmap().save(str(filepath))
            self._log(f"æˆªå›¾å·²ä¿å­˜: {filepath}")
            QMessageBox.information(self, "æˆªå›¾", f"æˆªå›¾å·²ä¿å­˜åˆ°:\n{filepath}")
    
    def _log(self, message: str):
        """æ·»åŠ æ—¥å¿—"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        self.log_text.append(f"[{timestamp}] {message}")
    
    def apply_theme(self, theme: str):
        """åº”ç”¨ä¸»é¢˜"""
        pass


# ============== MediaPipe ä¸“æ  ==============
class MediaPipePanel(QWidget):
    """MediaPipeå…³é”®ç‚¹æ£€æµ‹ä¸“æ """
    
    # ä¿¡å·
    inference_start_requested = pyqtSignal()
    inference_stop_requested = pyqtSignal()
    
    def __init__(self, config_manager: ConfigManager):
        super().__init__()
        
        self.config_manager = config_manager
        self.config = config_manager.get_config()
        
        self.mediapipe_thread: Optional[MediaPipeThread] = None
        self.is_running = False
        self.current_fps = 0.0
        self.detection_results: List[MediaPipeResult] = []
        
        self._init_ui()
    
    def _init_ui(self):
        """åˆå§‹åŒ–UI"""
        layout = QVBoxLayout(self)
        layout.setContentsMargins(5, 5, 5, 5)
        layout.setSpacing(10)
        
        # æ ‡é¢˜
        title_label = QLabel("ğŸ­ MediaPipe å…³é”®ç‚¹æ£€æµ‹")
        title_label.setStyleSheet("font-size: 16px; font-weight: bold; color: #2196f3;")
        title_label.setAlignment(Qt.AlignmentFlag.AlignCenter)
        layout.addWidget(title_label)
        
        # åˆ†å‰²å™¨
        splitter = QSplitter(Qt.Orientation.Horizontal)
        layout.addWidget(splitter)
        
        # å·¦ä¾§æ§åˆ¶é¢æ¿
        left_panel = self._create_left_panel()
        splitter.addWidget(left_panel)
        
        # å³ä¾§è§†é¢‘æ˜¾ç¤ºé¢æ¿
        right_panel = self._create_right_panel()
        splitter.addWidget(right_panel)
        
        # è®¾ç½®åˆ†å‰²æ¯”ä¾‹
        splitter.setSizes([350, 850])
    
    def _create_left_panel(self) -> QWidget:
        """åˆ›å»ºå·¦ä¾§æ§åˆ¶é¢æ¿"""
        panel = QWidget()
        layout = QVBoxLayout(panel)
        layout.setSpacing(10)
        
        # æ¨¡å‹è®¾ç½®ç»„
        model_group = QGroupBox("æ£€æµ‹æ¨¡å‹è®¾ç½®")
        model_layout = QVBoxLayout(model_group)
        
        # æ¨¡å‹çŠ¶æ€æ ‡ç­¾
        self.model_status_label = QLabel("æ¨¡å‹çŠ¶æ€: æœªåŠ è½½")
        self.model_status_label.setStyleSheet("color: #ff9800;")
        model_layout.addWidget(self.model_status_label)
        
        # å¯ç”¨å§¿æ€æ£€æµ‹
        self.enable_pose_check = QCheckBox("å¯ç”¨å§¿æ€æ£€æµ‹ (Pose)")
        self.enable_pose_check.setChecked(True)
        model_layout.addWidget(self.enable_pose_check)
        
        # å¯ç”¨æ‰‹éƒ¨æ£€æµ‹
        self.enable_hands_check = QCheckBox("å¯ç”¨æ‰‹éƒ¨æ£€æµ‹ (Hands)")
        self.enable_hands_check.stateChanged.connect(self._on_hands_check_changed)
        model_layout.addWidget(self.enable_hands_check)

        # å¯ç”¨æ‰‹åŠ¿è¯†åˆ«ï¼ˆä»…åœ¨æ‰‹éƒ¨æ£€æµ‹å¯ç”¨æ—¶å¯ç”¨ï¼‰
        self.enable_gesture_check = QCheckBox("  â””â”€ åŒæ—¶è¯†åˆ«æ‰‹åŠ¿ (Gesture)")
        self.enable_gesture_check.setEnabled(False)
        self.enable_gesture_check.setToolTip("è¯†åˆ«æ‰‹åŠ¿ç±»åˆ«ï¼šğŸ‘ ğŸ‘ âœŒï¸ â˜ï¸ âœŠ ğŸ‘‹")
        model_layout.addWidget(self.enable_gesture_check)

        # å¯ç”¨é¢éƒ¨æ£€æµ‹
        self.enable_face_check = QCheckBox("å¯ç”¨é¢éƒ¨æ£€æµ‹ (Face Mesh)")
        model_layout.addWidget(self.enable_face_check)
        
        # æ¨¡å‹å¤æ‚åº¦
        complexity_layout = QHBoxLayout()
        complexity_layout.addWidget(QLabel("æ¨¡å‹å¤æ‚åº¦:"))
        self.complexity_combo = QComboBox()
        self.complexity_combo.addItems(["è½»é‡ (0)", "æ ‡å‡† (1)", "é‡å‹ (2)"])
        self.complexity_combo.setCurrentIndex(1)
        complexity_layout.addWidget(self.complexity_combo)
        model_layout.addLayout(complexity_layout)
        
        # ç½®ä¿¡åº¦é˜ˆå€¼
        confidence_layout = QHBoxLayout()
        confidence_layout.addWidget(QLabel("ç½®ä¿¡åº¦é˜ˆå€¼:"))
        self.confidence_spin = QDoubleSpinBox()
        self.confidence_spin.setRange(0.1, 1.0)
        self.confidence_spin.setSingleStep(0.1)
        self.confidence_spin.setValue(0.5)
        confidence_layout.addWidget(self.confidence_spin)
        model_layout.addLayout(confidence_layout)
        
        layout.addWidget(model_group)
        
        # æ¨ç†æºè®¾ç½®ç»„
        source_group = QGroupBox("æ¨ç†æºè®¾ç½®")
        source_layout = QVBoxLayout(source_group)
        
        self.source_type_combo = QComboBox()
        self.source_type_combo.addItems(["æ‘„åƒå¤´", "æœ¬åœ°æ–‡ä»¶"])
        self.source_type_combo.currentIndexChanged.connect(self._on_source_type_changed)
        source_layout.addWidget(self.source_type_combo)
        
        self.source_config_widget = QWidget()
        self.source_config_layout = QVBoxLayout(self.source_config_widget)
        self.source_config_layout.setContentsMargins(0, 0, 0, 0)
        source_layout.addWidget(self.source_config_widget)
        
        # æ‘„åƒå¤´é…ç½®
        self.camera_config = QWidget()
        camera_layout = QHBoxLayout(self.camera_config)
        camera_layout.setContentsMargins(0, 0, 0, 0)
        camera_layout.addWidget(QLabel("æ‘„åƒå¤´ID:"))
        self.camera_id_spin = QSpinBox()
        self.camera_id_spin.setRange(0, 10)
        camera_layout.addWidget(self.camera_id_spin)
        camera_layout.addStretch()
        self.source_config_layout.addWidget(self.camera_config)
        
        # æ–‡ä»¶é…ç½®
        self.file_config = QWidget()
        file_layout = QHBoxLayout(self.file_config)
        file_layout.setContentsMargins(0, 0, 0, 0)
        self.file_path_input = QLineEdit()
        self.file_path_input.setPlaceholderText("é€‰æ‹©è§†é¢‘æˆ–å›¾åƒæ–‡ä»¶...")
        file_layout.addWidget(self.file_path_input)
        self.browse_file_btn = QPushButton("æµè§ˆ")
        self.browse_file_btn.clicked.connect(self._browse_file)
        file_layout.addWidget(self.browse_file_btn)
        self.source_config_layout.addWidget(self.file_config)
        self.file_config.setVisible(False)
        
        layout.addWidget(source_group)
        
        # æ§åˆ¶æŒ‰é’®
        btn_layout = QHBoxLayout()
        
        self.start_btn = QPushButton("å¼€å§‹æ£€æµ‹")
        self.start_btn.setStyleSheet("background-color: #4caf50; font-size: 14px; padding: 10px;")
        self.start_btn.clicked.connect(self._start_inference)
        btn_layout.addWidget(self.start_btn)
        
        self.stop_btn = QPushButton("åœæ­¢")
        self.stop_btn.setStyleSheet("background-color: #f44336; font-size: 14px; padding: 10px;")
        self.stop_btn.setEnabled(False)
        self.stop_btn.clicked.connect(self._stop_inference)
        btn_layout.addWidget(self.stop_btn)
        
        layout.addLayout(btn_layout)
        
        # æ•°æ®å¯¼å‡ºæŒ‰é’®
        export_layout = QHBoxLayout()
        
        self.export_json_btn = QPushButton("å¯¼å‡ºJSON")
        self.export_json_btn.clicked.connect(self._export_json)
        export_layout.addWidget(self.export_json_btn)
        
        self.export_csv_btn = QPushButton("å¯¼å‡ºCSV")
        self.export_csv_btn.clicked.connect(self._export_csv)
        export_layout.addWidget(self.export_csv_btn)
        
        layout.addLayout(export_layout)
        
        # æ¨ç†ç»Ÿè®¡
        stats_group = QGroupBox("æ£€æµ‹ç»Ÿè®¡")
        stats_layout = QVBoxLayout(stats_group)
        
        self.fps_label = QLabel("FPS: 0")
        stats_layout.addWidget(self.fps_label)
        
        self.pose_count_label = QLabel("å§¿æ€æ•°é‡: 0")
        stats_layout.addWidget(self.pose_count_label)
        
        self.hand_count_label = QLabel("æ‰‹éƒ¨æ•°é‡: 0")
        stats_layout.addWidget(self.hand_count_label)
        
        self.face_count_label = QLabel("é¢éƒ¨æ•°é‡: 0")
        stats_layout.addWidget(self.face_count_label)
        
        layout.addWidget(stats_group)
        
        layout.addStretch()
        return panel
    
    def _create_right_panel(self) -> QWidget:
        """åˆ›å»ºå³ä¾§æ˜¾ç¤ºé¢æ¿"""
        panel = QWidget()
        layout = QVBoxLayout(panel)
        layout.setSpacing(10)
        
        # è§†é¢‘æ˜¾ç¤ºæ ‡ç­¾
        self.video_label = QLabel()
        self.video_label.setMinimumSize(640, 480)
        self.video_label.setAlignment(Qt.AlignmentFlag.AlignCenter)
        self.video_label.setStyleSheet("background-color: #1a1a1a; border: 2px solid #555;")
        self.video_label.setText("ç­‰å¾…å¼€å§‹æ£€æµ‹...")
        layout.addWidget(self.video_label)
        
        # ç»“æœTab
        self.result_tabs = QTabWidget()
        
        # å…³é”®ç‚¹åˆ—è¡¨Tab
        self.keypoint_list = QTableWidget()
        self.keypoint_list.setColumnCount(5)
        self.keypoint_list.setHorizontalHeaderLabels(["ç±»å‹", "ID", "å…³é”®ç‚¹æ•°", "ç½®ä¿¡åº¦", "ä½ç½®"])
        self.keypoint_list.horizontalHeader().setStretchLastSection(True)
        self.result_tabs.addTab(self.keypoint_list, "å…³é”®ç‚¹åˆ—è¡¨")
        
        # æ—¥å¿—Tab
        self.log_text = QTextEdit()
        self.log_text.setReadOnly(True)
        self.result_tabs.addTab(self.log_text, "æ—¥å¿—")
        
        layout.addWidget(self.result_tabs)
        
        return panel
    
    def _on_source_type_changed(self, index: int):
        """æºç±»å‹æ”¹å˜"""
        self.camera_config.setVisible(index == 0)
        self.file_config.setVisible(index == 1)

    def _on_hands_check_changed(self, state: int):
        """æ‰‹éƒ¨æ£€æµ‹å¤é€‰æ¡†çŠ¶æ€æ”¹å˜"""
        self.enable_gesture_check.setEnabled(state == Qt.CheckState.Checked.value)
        if state != Qt.CheckState.Checked.value:
            self.enable_gesture_check.setChecked(False)

    def _browse_file(self):
        """æµè§ˆæ–‡ä»¶"""
        file_path, _ = QFileDialog.getOpenFileName(
            self, "é€‰æ‹©è§†é¢‘æˆ–å›¾åƒ", "", "è§†é¢‘/å›¾åƒ (*.mp4 *.avi *.jpg *.jpeg *.png);;æ‰€æœ‰æ–‡ä»¶ (*.*)"
        )
        if file_path:
            self.file_path_input.setText(file_path)
    
    def _start_inference(self):
        """å¼€å§‹æ¨ç†"""
        source_type = ["camera", "file"][self.source_type_combo.currentIndex()]
        source_config = {}
        
        if source_type == "camera":
            source_config['camera_id'] = self.camera_id_spin.value()
        elif source_type == "file":
            source_config['file_path'] = self.file_path_input.text()
        
        # è·å–æ¨¡å‹å¤æ‚åº¦ (0=è½»é‡, 1=æ ‡å‡†, 2=é‡å‹)
        model_complexity = self.complexity_combo.currentIndex()

        # å¯ç”¨æ‰‹åŠ¿è¯†åˆ«ï¼ˆä»…åœ¨æ‰‹éƒ¨æ£€æµ‹å¯ç”¨æ—¶ï¼‰
        enable_gesture = self.enable_hands_check.isChecked() and self.enable_gesture_check.isChecked()

        self.mediapipe_thread = MediaPipeThread(
            source_type=source_type,
            source_config=source_config,
            enable_pose=self.enable_pose_check.isChecked(),
            enable_hands=self.enable_hands_check.isChecked() and not enable_gesture,  # å¦‚æœå¯ç”¨æ‰‹åŠ¿è¯†åˆ«ï¼Œåˆ™ä¸å•ç‹¬ä½¿ç”¨æ‰‹éƒ¨æ£€æµ‹
            enable_face=self.enable_face_check.isChecked(),
            model_complexity=model_complexity,
            enable_gesture=enable_gesture
        )
        self.mediapipe_thread.frame_ready.connect(self._on_frame_ready)
        self.mediapipe_thread.fps_updated.connect(self._on_fps_updated)
        self.mediapipe_thread.error_occurred.connect(self._on_inference_error)
        self.mediapipe_thread.start()
        
        self.is_running = True
        self.start_btn.setEnabled(False)
        self.stop_btn.setEnabled(True)
        self.detection_results.clear()
        self._log(f"å¼€å§‹MediaPipeæ£€æµ‹: {source_type}")
    
    def stop_inference(self):
        """å…¬å…±æ–¹æ³•ï¼šåœæ­¢æ¨ç†"""
        self._stop_inference()
    
    def _stop_inference(self):
        """åœæ­¢æ¨ç†"""
        if self.mediapipe_thread:
            self.mediapipe_thread.stop()
            self.mediapipe_thread = None
        
        self.is_running = False
        self.start_btn.setEnabled(True)
        self.stop_btn.setEnabled(False)
        self.video_label.setText("æ£€æµ‹å·²åœæ­¢")
        self._log("æ£€æµ‹å·²åœæ­¢")
    
    def _on_frame_ready(self, frame: np.ndarray, result: MediaPipeResult):
        """å¸§å°±ç»ªå›è°ƒ"""
        try:
            if frame is None or frame.size == 0:
                return
            
            # è½¬æ¢OpenCVå›¾åƒä¸ºQPixmap
            rgb_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            h, w, ch = rgb_image.shape
            bytes_per_line = ch * w
            
            qt_image = QImage(rgb_image.data, w, h, bytes_per_line, QImage.Format.Format_RGB888)
            pixmap = QPixmap.fromImage(qt_image)
            
            scaled_pixmap = pixmap.scaled(
                self.video_label.size(),
                Qt.AspectRatioMode.KeepAspectRatio,
                Qt.TransformationMode.SmoothTransformation
            )
            
            self.video_label.setPixmap(scaled_pixmap)
            
            # ä¿å­˜ç»“æœç”¨äºå¯¼å‡º
            self.detection_results.append(result)
            
            # æ›´æ–°ç»Ÿè®¡
            self._update_stats(result)
            
        except Exception as e:
            logger.exception(f"å¤„ç†å¸§æ—¶å‡ºé”™: {e}")
    
    def _on_fps_updated(self, fps: float):
        """FPSæ›´æ–°å›è°ƒ"""
        self.current_fps = fps
        self.fps_label.setText(f"FPS: {fps:.1f}")
    
    def _on_inference_error(self, error: str):
        """æ¨ç†é”™è¯¯å›è°ƒ"""
        self._log(f"æ£€æµ‹é”™è¯¯: {error}")
        QMessageBox.critical(self, "æ£€æµ‹é”™è¯¯", error)
        self._stop_inference()
    
    def _on_fallback_mode(self, is_fallback: bool):
        """å¤‡ç”¨æ¨¡å¼åˆ‡æ¢å›è°ƒ"""
        if is_fallback:
            self._log("MediaPipeåŠ è½½å¤±è´¥ï¼Œå·²åˆ‡æ¢åˆ°OpenCVå¤‡ç”¨æ¨¡å¼")
            if hasattr(self, 'model_status_label'):
                self.model_status_label.setText("æ¨¡å‹çŠ¶æ€: OpenCVå¤‡ç”¨æ¨¡å¼")
                self.model_status_label.setStyleSheet("color: #ff9800;")
    
    def _update_stats(self, result: MediaPipeResult):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.pose_count_label.setText(f"å§¿æ€æ•°é‡: {len(result.poses)}")
        self.hand_count_label.setText(f"æ‰‹éƒ¨æ•°é‡: {len(result.hands)}")
        self.face_count_label.setText(f"é¢éƒ¨æ•°é‡: {len(result.faces)}")
        
        # æ›´æ–°å…³é”®ç‚¹åˆ—è¡¨
        total_items = len(result.poses) + len(result.hands) + len(result.faces)
        self.keypoint_list.setRowCount(total_items)
        
        row = 0
        for i, pose in enumerate(result.poses):
            self.keypoint_list.setItem(row, 0, QTableWidgetItem("å§¿æ€"))
            self.keypoint_list.setItem(row, 1, QTableWidgetItem(str(i)))
            self.keypoint_list.setItem(row, 2, QTableWidgetItem(str(len(pose.keypoints))))
            self.keypoint_list.setItem(row, 3, QTableWidgetItem(f"{pose.confidence:.2f}"))
            if pose.bbox:
                bbox_text = f"[{pose.bbox[0]:.0f}, {pose.bbox[1]:.0f}, {pose.bbox[2]:.0f}, {pose.bbox[3]:.0f}]"
                self.keypoint_list.setItem(row, 4, QTableWidgetItem(bbox_text))
            row += 1
        
        for i, hand in enumerate(result.hands):
            self.keypoint_list.setItem(row, 0, QTableWidgetItem("æ‰‹éƒ¨"))
            self.keypoint_list.setItem(row, 1, QTableWidgetItem(str(i)))
            self.keypoint_list.setItem(row, 2, QTableWidgetItem(str(len(hand.keypoints))))
            # å¦‚æœæœ‰æ‰‹åŠ¿ï¼Œæ˜¾ç¤ºæ‰‹åŠ¿ä¿¡æ¯
            if hand.gesture:
                self.keypoint_list.setItem(row, 3, QTableWidgetItem(f"{hand.gesture} ({hand.gesture_score:.2f})"))
            else:
                self.keypoint_list.setItem(row, 3, QTableWidgetItem("-"))
            row += 1
        
        for i, face in enumerate(result.faces):
            self.keypoint_list.setItem(row, 0, QTableWidgetItem("é¢éƒ¨"))
            self.keypoint_list.setItem(row, 1, QTableWidgetItem(str(i)))
            self.keypoint_list.setItem(row, 2, QTableWidgetItem(str(len(face))))
            self.keypoint_list.setItem(row, 3, QTableWidgetItem("-"))
            row += 1
    
    def _export_json(self):
        """å¯¼å‡ºä¸ºJSONæ ¼å¼"""
        if not self.detection_results:
            QMessageBox.warning(self, "è­¦å‘Š", "æ²¡æœ‰æ£€æµ‹ç»“æœå¯å¯¼å‡º")
            return
        
        file_path, _ = QFileDialog.getSaveFileName(
            self, "å¯¼å‡ºJSON", "mediapipe_results.json", "JSONæ–‡ä»¶ (*.json)"
        )
        if not file_path:
            return
        
        try:
            export_data = []
            for result in self.detection_results:
                frame_data = {
                    "fps": result.fps,
                    "inference_time": result.inference_time,
                    "poses": [],
                    "hands": [],
                    "faces": []
                }
                
                for pose in result.poses:
                    frame_data["poses"].append({
                        "confidence": pose.confidence,
                        "keypoints": [{"x": kp.x, "y": kp.y, "z": kp.z, "visibility": kp.visibility} 
                                     for kp in pose.keypoints]
                    })
                
                for hand in result.hands:
                    frame_data["hands"].append([
                        {"x": kp.x, "y": kp.y, "z": kp.z} for kp in hand
                    ])
                
                for face in result.faces:
                    frame_data["faces"].append([
                        {"x": kp.x, "y": kp.y, "z": kp.z} for kp in face
                    ])
                
                export_data.append(frame_data)
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, indent=2, ensure_ascii=False)
            
            self._log(f"ç»“æœå·²å¯¼å‡ºåˆ°: {file_path}")
            QMessageBox.information(self, "å¯¼å‡ºæˆåŠŸ", f"ç»“æœå·²å¯¼å‡ºåˆ°:\n{file_path}")
            
        except Exception as e:
            logger.exception(f"å¯¼å‡ºJSONå¤±è´¥: {e}")
            QMessageBox.critical(self, "å¯¼å‡ºå¤±è´¥", f"å¯¼å‡ºå¤±è´¥:\n{e}")
    
    def _export_csv(self):
        """å¯¼å‡ºä¸ºCSVæ ¼å¼"""
        if not self.detection_results:
            QMessageBox.warning(self, "è­¦å‘Š", "æ²¡æœ‰æ£€æµ‹ç»“æœå¯å¯¼å‡º")
            return
        
        file_path, _ = QFileDialog.getSaveFileName(
            self, "å¯¼å‡ºCSV", "mediapipe_results.csv", "CSVæ–‡ä»¶ (*.csv)"
        )
        if not file_path:
            return
        
        try:
            with open(file_path, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(["frame", "type", "id", "kp_id", "x", "y", "z", "visibility"])
                
                for frame_idx, result in enumerate(self.detection_results):
                    for pose_idx, pose in enumerate(result.poses):
                        for kp_idx, kp in enumerate(pose.keypoints):
                            writer.writerow([
                                frame_idx, "pose", pose_idx, kp_idx,
                                kp.x, kp.y, kp.z, kp.visibility
                            ])
                    
                    for hand_idx, hand in enumerate(result.hands):
                        for kp_idx, kp in enumerate(hand):
                            writer.writerow([
                                frame_idx, "hand", hand_idx, kp_idx,
                                kp.x, kp.y, kp.z, ""
                            ])
                    
                    for face_idx, face in enumerate(result.faces):
                        for kp_idx, kp in enumerate(face):
                            writer.writerow([
                                frame_idx, "face", face_idx, kp_idx,
                                kp.x, kp.y, kp.z, ""
                            ])
            
            self._log(f"ç»“æœå·²å¯¼å‡ºåˆ°: {file_path}")
            QMessageBox.information(self, "å¯¼å‡ºæˆåŠŸ", f"ç»“æœå·²å¯¼å‡ºåˆ°:\n{file_path}")
            
        except Exception as e:
            logger.exception(f"å¯¼å‡ºCSVå¤±è´¥: {e}")
            QMessageBox.critical(self, "å¯¼å‡ºå¤±è´¥", f"å¯¼å‡ºå¤±è´¥:\n{e}")
    
    def _log(self, message: str):
        """æ·»åŠ æ—¥å¿—"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        self.log_text.append(f"[{timestamp}] {message}")
    
    def apply_theme(self, theme: str):
        """åº”ç”¨ä¸»é¢˜"""
        pass


# ============== ä¸»æ¨ç†é¢æ¿ ==============
class InferenceWidget(QWidget):
    """æ¨ç†é¢æ¿ - åŒ…å«YOLOå’ŒMediaPipeä¸¤ä¸ªä¸“æ """
    
    # ä¿¡å·
    inference_start_requested = pyqtSignal()
    inference_stop_requested = pyqtSignal()
    model_load_requested = pyqtSignal(str, int)  # æ¨¡å‹è·¯å¾„, ä»»åŠ¡ç±»å‹
    
    def __init__(self, config_manager: ConfigManager):
        super().__init__()
        
        self.config_manager = config_manager
        self.config = config_manager.get_config()
        
        self._init_ui()
    
    def _init_ui(self):
        """åˆå§‹åŒ–UI"""
        layout = QVBoxLayout(self)
        layout.setContentsMargins(0, 0, 0, 0)
        layout.setSpacing(0)
        
        # åˆ›å»ºä¸“æ åˆ‡æ¢Tab
        self.column_tabs = QTabWidget()
        self.column_tabs.setDocumentMode(True)
        self.column_tabs.setTabPosition(QTabWidget.TabPosition.North)
        
        # YOLOä¸“æ 
        self.yolo_panel = YOLOPanel(self.config_manager)
        self.yolo_panel.model_load_requested.connect(self.model_load_requested)
        self.yolo_panel.inference_start_requested.connect(self.inference_start_requested)
        self.yolo_panel.inference_stop_requested.connect(self.inference_stop_requested)
        self.column_tabs.addTab(self.yolo_panel, "ğŸ¯ YOLOç›®æ ‡æ£€æµ‹")
        
        # MediaPipeä¸“æ 
        self.mediapipe_panel = MediaPipePanel(self.config_manager)
        self.mediapipe_panel.inference_start_requested.connect(self.inference_start_requested)
        self.mediapipe_panel.inference_stop_requested.connect(self.inference_stop_requested)
        self.column_tabs.addTab(self.mediapipe_panel, "ğŸ­ MediaPipeå…³é”®ç‚¹æ£€æµ‹")
        
        layout.addWidget(self.column_tabs)
    
    def on_model_loaded(self, success: bool):
        """æ¨¡å‹åŠ è½½å›è°ƒ - è½¬å‘ç»™YOLOé¢æ¿"""
        self.yolo_panel.on_model_loaded(success)
    
    def set_inference_engine(self, engine: YOLOInference):
        """è®¾ç½®æ¨ç†å¼•æ“ - è½¬å‘ç»™YOLOé¢æ¿"""
        self.yolo_panel.set_inference_engine(engine)
    
    def apply_theme(self, theme: str):
        """åº”ç”¨ä¸»é¢˜"""
        self.yolo_panel.apply_theme(theme)
        self.mediapipe_panel.apply_theme(theme)
    
    def start_inference(self):
        """å¤–éƒ¨è°ƒç”¨å¼€å§‹æ¨ç†"""
        current_tab = self.column_tabs.currentIndex()
        if current_tab == 0:
            self.yolo_panel.start_inference()
        else:
            self.mediapipe_panel.start_inference()
    
    def stop_inference(self):
        """å¤–éƒ¨è°ƒç”¨åœæ­¢æ¨ç†"""
        self.yolo_panel.stop_inference()
        self.mediapipe_panel.stop_inference()
